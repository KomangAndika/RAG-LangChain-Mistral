{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8171662,"sourceType":"datasetVersion","datasetId":4836341},{"sourceId":5112,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3900}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -qU langchain accelerate bitsandbytes transformers sentence-transformers faiss-gpu","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U langchain-community","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## RAG(Retrieval-Augmented Generation) with LangChain\nRag typically has two main components:\n1. Indexing, basically pipeline to ingest data (Usually done Offline).\n2. Retrieival + Generation, this is the actual part of the RAG, receive user query and retrieve relevant data from index and passing the model. <br>\n### Indexing\n- Loading Document, using document loaders, can be from Google Drive, Notion, Slack, but in this case I am using kaggle database(or basically local kaggle notebook).\n- Split, using text splitter to break documents into smaller chunks, useful for indexing and feeding the model.\n- Store, place to store and index the splits (VectorDB and Embedding model are here), the VectorDB I am using is FAISS and sentence-transformer for embedding.\n\n### Retrieval + Generation\n- Retrieve, given the user input, retrieve relevant splits from the VectorDB.\n- Generate, using chatmodel/LLM(in this case Mistral) to produce answer using prompt and retrieval data","metadata":{}},{"cell_type":"code","source":"import os\nimport transformers\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n)\n\nfrom langchain.document_loaders import TextLoader\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS #VectorDB\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.chains import LLMChain\nfrom langchain.schema.runnable import RunnablePassthrough\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.prompts import PromptTemplate\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom glob import glob","metadata":{"execution":{"iopub.status.busy":"2024-07-04T07:11:43.853489Z","iopub.execute_input":"2024-07-04T07:11:43.853881Z","iopub.status.idle":"2024-07-04T07:11:44.155314Z","shell.execute_reply.started":"2024-07-04T07:11:43.853845Z","shell.execute_reply":"2024-07-04T07:11:44.154586Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#Reducing Model size to save memory and increasing speed\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=\"float16\",\n    bnb_4bit_use_double_quant=False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T07:12:00.846165Z","iopub.execute_input":"2024-07-04T07:12:00.847022Z","iopub.status.idle":"2024-07-04T07:12:00.853189Z","shell.execute_reply.started":"2024-07-04T07:12:00.846983Z","shell.execute_reply":"2024-07-04T07:12:00.852152Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Model Pipeline and Embedding Initialization \nmodel = AutoModelForCausalLM.from_pretrained(\n    \"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\", \n    quantization_config = bnb_config,\n    do_sample=True,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\")\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\ntext_generation_pipeline = transformers.pipeline(\n    model=model,\n    tokenizer=tokenizer,\n    temperature=0.7,    \n    task=\"text-generation\",\n    repetition_penalty=1.1,\n    return_full_text=True,\n    max_new_tokens=2000,    \n)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T07:13:06.220201Z","iopub.execute_input":"2024-07-04T07:13:06.220570Z","iopub.status.idle":"2024-07-04T07:16:16.070033Z","shell.execute_reply.started":"2024-07-04T07:13:06.220541Z","shell.execute_reply":"2024-07-04T07:16:16.069277Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f8978bb9e93457fbf2e7b4bb2aff0ea"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n2024-07-04 07:16:05.276686: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-04 07:16:05.276807: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-04 07:16:05.450848: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n\nprompt_template = \"\"\"\nInstruction: Answer the question based on the following context:\n{context}\n\nQuestion:\n{question} \n \"\"\"\n\n# Create prompt from prompt template \nprompt = PromptTemplate(\n    input_variables=[\"context\", \"question\"],\n    template=prompt_template,\n)\n\n# Create llm chain \nllm_chain = LLMChain(llm=mistral_llm, prompt=prompt)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T07:18:20.198582Z","iopub.execute_input":"2024-07-04T07:18:20.199879Z","iopub.status.idle":"2024-07-04T07:18:20.612288Z","shell.execute_reply.started":"2024-07-04T07:18:20.199840Z","shell.execute_reply":"2024-07-04T07:18:20.611360Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n  warn_deprecated(\n/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n  warn_deprecated(\n","output_type":"stream"}]},{"cell_type":"code","source":"paper_paths = glob(\"/kaggle/input/10-transformative-llm-research-papers-of-2023/LLM Research Papers of 2023/*.pdf\")\npages = []\n\nfor path in paper_paths:\n    try:\n        loader = PyPDFLoader(path)\n        doc = loader.load()\n        text_splitter = CharacterTextSplitter(chunk_size=500, \n                                      chunk_overlap=0)\n        chunked_documents = text_splitter.split_documents(doc)\n        \n        pages.extend(chunked_documents)\n    except Exception as e:\n        print('Skipping', path, e)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T07:20:54.110681Z","iopub.execute_input":"2024-07-04T07:20:54.111392Z","iopub.status.idle":"2024-07-04T07:21:34.711255Z","shell.execute_reply.started":"2024-07-04T07:20:54.111359Z","shell.execute_reply":"2024-07-04T07:21:34.710298Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Load chunked documents into the FAISS index\ndb = FAISS.from_documents(\n    pages,\n    HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2')\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T07:21:46.677959Z","iopub.execute_input":"2024-07-04T07:21:46.678643Z","iopub.status.idle":"2024-07-04T07:22:05.865701Z","shell.execute_reply.started":"2024-07-04T07:21:46.678609Z","shell.execute_reply":"2024-07-04T07:22:05.864914Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n  warn_deprecated(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bec9983c01f74a8d89ee151ca2b25390"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3c08aa2003b4458a21c6710aa18ea46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b350e765883b49dd9e25566d1c670239"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c7602393fc848ffb04094f985255c71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f5de6ffd69e492c8c49283714c6ec36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c78e40027662432f85e82f502cc59698"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"129fb1729cbf4535953b464c2ecf5b42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22a0cdfaf83246bb91c85b565a1f676e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb776be56a0d4f09a0e4edb4b1cd7905"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0601f6426ba44ab7afce00d2b55510e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7518a9297834647abe3651c9fb92cb9"}},"metadata":{}}]},{"cell_type":"code","source":"retriever = db.as_retriever()\n\nrag_chain = (\n {\"context\": retriever, \"question\": RunnablePassthrough()}\n    | llm_chain\n)\n\nresponse = rag_chain.invoke(\"Why are vision-language task more diverse than NLP task?\")\n\nprint (\"Question:\", response[\"question\"])\nprint (response['text'])","metadata":{"execution":{"iopub.status.busy":"2024-07-04T07:41:10.391422Z","iopub.execute_input":"2024-07-04T07:41:10.392110Z","iopub.status.idle":"2024-07-04T07:41:19.333955Z","shell.execute_reply.started":"2024-07-04T07:41:10.392076Z","shell.execute_reply":"2024-07-04T07:41:19.332966Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Question: Why are vision-language task more diverse than NLP task?\n\nInstruction: Answer the question based on the following context:\n[Document(metadata={'source': '/kaggle/input/10-transformative-llm-research-papers-of-2023/LLM Research Papers of 2023/2303.12712.pdf', 'page': 12}, page_content='Figure 2.1: The ﬁrst image is Composition 8, art by Wassily Kandinsky, the second and the third\\nare produced by GPT-4 and ChatGPT respectively with the prompt “Produce Javacript code that\\ncreates a random graphical image that looks like a painting of Kandinsky”.\\n2 Multimodal and interdisciplinary composition\\nA key measure of intelligence is the ability to synthesize information from diﬀerent domains or modalities\\nand the capacity to apply knowledge and skills across diﬀerent contexts or disciplines. In this section we will\\nsee that, not only does GPT-4 demonstrate a high level of proﬁciency in diﬀerent domains such as literature,\\nmedicine, law, mathematics, physical sciences, and programming, but it is also able to combine skills and\\nconcepts from multiple domains with ﬂuidity, showing an impressive comprehension of complex ideas . In\\naddition to natural language experiments we also explore two perhaps unexpected modalities for a language\\nmodel (as explained in the introduction, we emphasize again that our experiments were done on an early\\nversion of GPT-4 which was notmultimodal) with vision in Section 2.2 and audio in Section 2.3.\\n2.1 Integrative ability\\nTo showcase the model’s remarkable integrative ability, we start with several examples that require generating\\ntext and code in a way that combines knowledge or skills from multiple disciplines. We deliberately picked\\ncombinations of domains that the training data would rarely include, such as literature and mathematics or\\nprogramming and art.\\n1. In order to test the model’s ability to combine capabilities in art and programming, we ask GPT-4 to\\n“Produce javascript code which generates random images in the style of the painter Kandinsky”. See a\\nsample image and the code in Figure 2.1 and Figure B.1.\\n2. The model was able to produce a proof of the fact there are inﬁnitely many prime numbers in the\\nliterary style of Shakespeare (Figure 2.2).\\n3. We tested the model’s ability to combine knowledge in history and physics by asking it to write a\\nsupporting letter for Electron as a US presidential candidate, written by Mahatma Gandhi and addressed\\nto his wife (Figure 2.3).\\n4. We prompted the model to “Produce python code for a program that takes as an input a patient’s\\nage, sex, weight, height and blood test results vector and indicates if the person is at increased risk for\\ndiabetes”, which resulted in the code appearing in Figure B.3.\\nThese examples suggest that GPT-4 has not only learned some general principles and patterns of diﬀerent\\ndomains and styles but can also synthesize them in creative and novel ways. These interdisciplinary skills are\\nnot unique to GPT-4. ChatGPT can also produce answers that show some understanding of the task and\\nthe domains involved (see Figures 2.2, B.2, B.3), but they are often incomplete and, arguably, considerably\\nless creative. For example, in Figure 2.3, GPT-4 outperforms ChatGPT in several aspects as it correctly\\npersonalizes the letter according to the referee (Gandhi), the recipient (his wife), the candidate (Electron),\\nand the job (US president). We do not claim to have a precise method for evaluating the results of these tasks\\nor a rigorous comparison between the two models, but we want to give the reader a sense of how the two\\nmodels diﬀer (note that we also ask GPT-4 directly to evaluate the diﬀerence, see Figure 2.2 and Figure 2.3).\\n13'), Document(metadata={'source': '/kaggle/input/10-transformative-llm-research-papers-of-2023/LLM Research Papers of 2023/2302.13971.pdf', 'page': 6}, page_content='Humanities STEM Social Sciences Other Average\\nGPT-NeoX 20B 29.8 34.9 33.7 37.7 33.6\\nGPT-3 175B 40.8 36.7 50.4 48.8 43.9\\nGopher 280B 56.2 47.4 71.9 66.1 60.0\\nChinchilla 70B 63.6 54.9 79.3 73.9 67.5\\nPaLM8B 25.6 23.8 24.1 27.8 25.4\\n62B 59.5 41.9 62.7 55.8 53.7\\n540B 77.0 55.6 81.0 69.6 69.3\\nLLaMA7B 34.0 30.5 38.3 38.1 35.1\\n13B 45.0 35.8 53.8 53.3 46.9\\n33B 55.8 46.0 66.7 63.4 57.8\\n65B 61.8 51.7 72.9 67.4 63.4\\nTable 9: Massive Multitask Language Understanding (MMLU). Five-shot accuracy.\\nthat may indicate that this benchmark is not\\nreliable. On WinoGrande, the performance does\\nnot correlate as well with training perplexity:\\nthe LLaMA-33B and LLaMA-65B have similar\\nperformance during the training.\\n4 Instruction Finetuning\\nIn this section, we show that brieﬂy ﬁnetuning on\\ninstructions data rapidly leads to improvements\\non MMLU. Although the non-ﬁnetuned version\\nof LLaMA-65B is already able to follow basic in-\\nstructions, we observe that a very small amount of\\nﬁnetuning improves the performance on MMLU,\\nand further improves the ability of the model to\\nfollow instructions. Since this is not the focus of\\nthis paper, we only conducted a single experiment\\nfollowing the same protocol as Chung et al. (2022)\\nto train an instruct model, LLaMA-I.\\nOPT 30B 26.1\\nGLM 120B 44.8\\nPaLM 62B 55.1\\nPaLM-cont 62B 62.8\\nChinchilla 70B 67.5\\nLLaMA 65B 63.4\\nOPT-IML-Max 30B 43.2\\nFlan-T5-XXL 11B 55.1\\nFlan-PaLM 62B 59.6\\nFlan-PaLM-cont 62B 66.1\\nLLaMA-I 65B 68.9\\nTable 10: Instruction ﬁnetuning – MMLU (5-shot).\\nComparison of models of moderate size with and with-\\nout instruction ﬁnetuning on MMLU.In Table 10, we report the results of our instruct\\nmodel LLaMA-I on MMLU and compare with ex-\\nisting instruction ﬁnetuned models of moderate\\nsizes, namely, OPT-IML (Iyer et al., 2022) and the\\nFlan-PaLM series (Chung et al., 2022). All the re-\\nported numbers are from the corresponding papers.\\nDespite the simplicity of the instruction ﬁnetuning\\napproach used here, we reach 68.9% on MMLU.\\nLLaMA-I (65B) outperforms on MMLU existing\\ninstruction ﬁnetuned models of moderate sizes, but\\nare still far from the state-of-the-art, that is 77.4\\nfor GPT code-davinci-002 on MMLU (numbers\\ntaken from Iyer et al. (2022)). The details of the\\nperformance on MMLU on the 57 tasks can be\\nfound in Table 16 of the appendix.\\n5 Bias, Toxicity and Misinformation\\nLarge language models have been showed to re-\\nproduce and amplify biases that are existing in\\nthe training data (Sheng et al., 2019; Kurita et al.,\\n2019), and to generate toxic or offensive con-\\ntent (Gehman et al., 2020). As our training dataset\\ncontains a large proportion of data from the Web,\\nwe believe that it is crucial to determine the po-\\ntential for our models to generate such content.\\nTo understand the potential harm of LLaMA-65B,\\nwe evaluate on different benchmarks that measure\\ntoxic content production and stereotypes detection.\\nWhile we have selected some of the standard bench-\\nmarks that are used by the language model com-\\nmunity to indicate some of the issues with these\\nmodels, these evaluations are not sufﬁcient to fully\\nunderstand the risks associated with these models.'), Document(metadata={'source': '/kaggle/input/10-transformative-llm-research-papers-of-2023/LLM Research Papers of 2023/2305.06500.pdf', 'page': 0}, page_content='InstructBLIP: Towards General-purpose\\nVision-Language Models with Instruction Tuning\\nWenliang Dai†1,2∗Junnan Li†,B,1Dongxu Li1Anthony Meng Huat Tiong1,3\\nJunqi Zhao3Weisheng Wang3Boyang Li3Pascale Fung2Steven HoiB,1\\n1Salesforce Research2Hong Kong University of Science and Technology\\n3Nanyang Technological University, Singapore\\nhttps://github.com/salesforce/LAVIS/tree/main/projects/instructblip\\n†Equal contributionBCorresponding authors: {junnan.li,shoi@salesforce.com}\\nAbstract\\nLarge-scale pre-training and instruction tuning have been successful at creating\\ngeneral-purpose language models with broad competence. However, building\\ngeneral-purpose vision-language models is challenging due to the rich input dis-\\ntributions and task diversity resulting from the additional visual input. Although\\nvision-language pretraining has been widely studied, vision-language instruction\\ntuning remains under-explored. In this paper, we conduct a systematic and compre-\\nhensive study on vision-language instruction tuning based on the pretrained BLIP-2\\nmodels. We gather 26 publicly available datasets, covering a wide variety of tasks\\nand capabilities, and transform them into instruction tuning format. Additionally,\\nwe introduce an instruction-aware Query Transformer, which extracts informative\\nfeatures tailored to the given instruction. Trained on 13 held-in datasets, Instruct-\\nBLIP attains state-of-the-art zero-shot performance across all 13 held-out datasets,\\nsubstantially outperforming BLIP-2 and larger Flamingo models. Our models\\nalso lead to state-of-the-art performance when finetuned on individual downstream\\ntasks (e.g., 90.7% accuracy on ScienceQA questions with image contexts). Further-\\nmore, we qualitatively demonstrate the advantages of InstructBLIP over concurrent\\nmultimodal models. All InstructBLIP models are open-sourced.\\n1 Introduction\\nA longstanding aspiration of Artificial Intelligence (AI) research is to build a single model that\\ncan solve arbitrary tasks specified by the user. In natural language processing (NLP), instruction\\ntuning [ 46,7] proves to be a promising approach toward that goal. By finetuning a large language\\nmodel (LLM) on a wide range of tasks described by natural language instructions, instruction tuning\\nenables the model to follow arbitrary instructions. Recently, instruction-tuned LLMs have also been\\nleveraged for vision-language tasks. For example, BLIP-2 [ 20] effectively adapts frozen instruction-\\ntuned LLMs to understand visual inputs and exhibits preliminary capabilities to follow instructions in\\nimage-to-text generation.\\nCompared to NLP tasks, vision-language tasks are more diverse in nature due to the additional visual\\ninputs from various domains. This poses a greater challenge to a unified model that is supposed to\\ngeneralize to diverse vision-language tasks, many unseen during training. Most previous work can\\nbe grouped into two approaches. The first approach, multitask learning [ 6,27], formulates various\\nvision-language tasks into the same input-output format. However, we empirically find multitask\\nlearning without instructions (Table 4) does not generalize well to unseen datasets and tasks. The\\n∗Work done during internship at Salesforce.\\nPreprint. Under review.arXiv:2305.06500v2  [cs.CV]  15 Jun 2023'), Document(metadata={'source': '/kaggle/input/10-transformative-llm-research-papers-of-2023/LLM Research Papers of 2023/2303.08774.pdf', 'page': 7}, page_content='0% 10% 20% 30% 40% 50% 60% 70% 80% 90%\\nAccuracy →GPT-4 3-shot accuracy on MMLU across languages\\nRandom\\nChinchilla\\nPaLM\\ngpt-3.5\\ngpt-425.0%\\n67.0%\\n69.3%\\n70.1%\\n85.5%\\n84.1%\\n84.1%\\n84.0%\\n83.7%\\n83.6%\\n83.1%\\n82.7%\\n82.1%\\n81.9%\\n81.4%\\n80.9%\\n80.1%\\n80.0%\\n80.0%\\n79.9%\\n78.5%\\n77.5%\\n77.0%\\n76.5%\\n73.2%\\n72.6%\\n72.2%\\n71.8%\\n71.4%\\n66.7%\\n62.0%Random guessing\\nChinchilla-English\\nPaLM-English\\nGPT-3.5-English\\nGPT-4 English\\nItalian\\nAfrikaans\\nSpanish\\nGerman\\nFrench\\nIndonesian\\nRussian\\nPolish\\nUkranian\\nGreek\\nLatvian\\nMandarin\\nArabic\\nTurkish\\nJapanese\\nSwahili\\nWelsh\\nKorean\\nIcelandic\\nBengali\\nUrdu\\nNepali\\nThai\\nPunjabi\\nMarathi\\nTeluguFigure 5. Performance of GPT-4 in a variety of languages compared to prior models in English on\\nMMLU. GPT-4 outperforms the English-language performance of existing language models [ 2,3] for\\nthe vast majority of languages tested, including low-resource languages such as Latvian, Welsh, and\\nSwahili.\\nto increase the diversity of these benchmarks over time to represent a wider set of failure modes and\\na harder set of tasks.\\n4.1 Visual Inputs\\nGPT-4 accepts prompts consisting of both images and text, which – parallel to the text-only setting\\n– lets the user specify any vision or language task. Specifically, the model generates text outputs\\ngiven inputs consisting of arbitrarily interlaced text and images. Over a range of domains – including\\ndocuments with text and photographs, diagrams, or screenshots – GPT-4 exhibits similar capabilities\\nas it does on text-only inputs. An example of GPT-4’s visual input can be found in Table 3. The\\nstandard test-time techniques developed for language models (e.g. few-shot prompting, chain-of-\\nthought, etc) are similarly effective when using both images and text - see Appendix G for examples.\\nPreliminary results on a narrow set of academic vision benchmarks can be found in the GPT-4 blog\\npost [ 65]. We plan to release more information about GPT-4’s visual capabilities in follow-up work.\\n8')]\n\nQuestion:\nWhy are vision-language task more diverse than NLP task? \n \nAnswer:\nVision-language tasks are more diverse than NLP tasks because they involve additional visual inputs from various domains. This poses a greater challenge to a unified model that is supposed to generalize to diverse vision-language tasks, many unseen during training.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}